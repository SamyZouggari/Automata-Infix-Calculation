üá¨üáß:
# Automata-Driven Data and Expression Analysis
In this project, I developed several Python scripts by implementing automata to process complex data and mathematical expressions. The approach was structured, allowing me to explore different aspects of data processing and validation through multiple stages.

First, I implemented automata capable of recognizing and validating different types of numeric values. These automata are designed to distinguish and process integers, decimal numbers, as well as numbers written with exponents. Their purpose is to ensure that these numeric values adhere to a predefined format and provide a robust tool for validating numeric data in various contexts.

Next, I designed an infix calculator to manipulate and evaluate mathematical expressions. Unlike prefix or postfix notations, infix notation is the most common and uses the traditional order of operators, as in a standard expression. Developing this calculator allowed me to work on the analysis and interpretation of infix expressions, correctly handling operator precedence and resolving expressions in the proper order. This project was an excellent exercise in applying algorithms and managing mathematical logic.

Finally, I integrated a script to separate and analyze lexemes and tokens. This process, known as lexing, forms the first stage of compilation or syntactic analysis, breaking down a string of instructions into distinct lexical units (tokens). This step is crucial for interpreting instructions, as it separates the different components of an expression or program, facilitating future analyses. The script checks for the presence or absence of specific tokens, validating the instructions according to defined criteria.

Overall, this project was a rich experience that allowed me to deepen my skills in algorithm design, numeric data validation, and syntactic analysis. By using automata, I was able to create tools capable of processing and interpreting complex data, while strengthening my programming knowledge and analysis of mathematical expressions. It also gave me the opportunity to apply my theoretical knowledge in formal language theory, particularly regular and context-free grammars.

üá´üá∑:
# Analyse des Donn√©es et Expressions via Automates
Dans ce projet, j'ai d√©velopp√© plusieurs scripts en Python en mettant en ≈ìuvre des automates pour traiter des donn√©es et des expressions math√©matiques complexes. L'approche a √©t√© structur√©e, me permettant d'explorer diff√©rentes facettes du traitement et de la validation des donn√©es √† travers plusieurs √©tapes.

Tout d'abord, j'ai impl√©ment√© des automates capables de reconna√Ætre et de valider diff√©rents types de valeurs num√©riques. Ces automates sont con√ßus pour distinguer et traiter les entiers, les nombres d√©cimaux, ainsi que les nombres √©crits avec des exposants. Leur objectif est de s'assurer que ces valeurs num√©riques respectent un format pr√©d√©fini et de fournir un outil robuste pour la validation des donn√©es num√©riques dans divers contextes.

Ensuite, j'ai con√ßu une calculatrice infixe qui permet de manipuler et d'√©valuer des expressions math√©matiques. Contrairement aux notations pr√©fixe ou postfixe, la notation infixe est la plus courante et utilise l'ordre traditionnel des op√©rateurs, comme dans une expression classique. Le d√©veloppement de cette calculatrice m'a permis de travailler sur l'analyse et l'interpr√©tation des expressions infixes, en traitant correctement les priorit√©s des op√©rateurs et en r√©solvant les expressions dans l'ordre appropri√©. Ce projet a √©t√© un excellent exercice de mise en pratique des algorithmes et de la gestion de la logique math√©matique.

Enfin, j'ai int√©gr√© un script permettant de s√©parer et d'analyser les lex√®mes et les tokens. Ce processus de lexing, qui constitue la premi√®re √©tape de la compilation ou de l'analyse syntaxique, permet de d√©composer une cha√Æne d'instructions en unit√©s lexicales distinctes (les tokens). Cette √©tape est cruciale pour l'interpr√©tation des instructions, car elle permet de s√©parer les diff√©rents composants d'une expression ou d'un programme afin de faciliter les analyses futures. Le script v√©rifie la pr√©sence ou l'absence de tokens sp√©cifiques, validant ainsi les instructions selon les crit√®res d√©finis.

Dans l'ensemble, ce projet a √©t√© une exp√©rience riche qui m'a permis d'approfondir mes comp√©tences en algorithmie, en validation des donn√©es num√©riques, et en analyse syntaxique. Gr√¢ce √† l'utilisation d'automates, j'ai pu cr√©er des outils capables de traiter et d'interpr√©ter des donn√©es complexes, tout en renfor√ßant mes connaissances en programmation et en analyse d'expressions math√©matiques. Cela a aussi permit de mettre en pratique mes connaissances th√©oriques en th√©orie des langages, particuli√®rement les grammaires r√©guli√®res et hors-contexte.
